{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Computacional 3. Rede Convolucional e Transfer Learning\n",
    "\n",
    "## 1. Introdução e Base de Dados\n",
    "\n",
    "Neste trabalho usaremos uma rede convolucional pré-treinada e a aplicaremos em um problema novo. Também experimentaremos com a\n",
    "divisão da base em treinamento, validação e teste, e usaremos validação para o \"early stopping\" na tentativa de controlar o sobre-ajuste.\n",
    "Aproveitamos código publicado na internet por Gabriel Cassimiro.\n",
    "\n",
    "A base de dados é a \"TensorFlow Flowers Dataset\". Ela contém 3670 imagens coloridas de flores pertencentes a uma de 5 classes: Margarida,\n",
    "Dente-de-leão, Rosa, Girassol e Tulipa.\n",
    "\n",
    "Ela pode ser baixada com o código abaixo.\n",
    "\n",
    "Obs.: o módulo `tensorflow_datasets` a princípio não é acessível em instalações Windows. Você pode usar uma instalação local Unix, ou um\n",
    "serviço de núvem como o Google Colab ou Amazon Web Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "337cb9bac56348ecb28cf342cbf6b204",
      "efd38707eb0a4ac4b875626cd1f89eed",
      "ccd55a62cefb468c8a2483fd840744ad",
      "006e39be5baa48efa5f6a764794ac88a",
      "56a44b3f23a145599a1ead5f4863ae5a",
      "f2014973acc24cc0a2b740e53557f1b7",
      "1cfef9f82b7d49898c0e8ae5846d4f29",
      "89dc8b13a44748378faca300b8431842",
      "e7973d1aef464f82a1b24f67028f5720",
      "831cd354b1e940029462d8ca6bbbb2da",
      "522ff35e6a3b48789e4f907fa64c6267"
     ]
    },
    "executionInfo": {
     "elapsed": 14971,
     "status": "ok",
     "timestamp": 1699914359672,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "Ef2uTTEXumSC",
    "outputId": "ba7d59ee-bfbe-46df-bc71-c915f27d5b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2569, 150, 150, 3)\n",
      "(1101, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "## Loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[:30%]\"], ## Train test split\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "## Resizing images\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))\n",
    "\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "print(train_ds.shape)\n",
    "print(test_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como foi feita a separação em 70% de dados para treinamento e 30% de dados para teste. Observe também que a base de dados\n",
    "original tem apenas um conjunto \"train\". A separação em treinamento, teste e validação é feita pelo usuário. Por isso a instrução para obter\n",
    "70% do conjunto \"train\" e 30% do conjunto \"train\", o que soa estranho a princípio.\n",
    "\n",
    "## 2. Treinando um MLP\n",
    "\n",
    "Use esta base de dados para treinar um MLP , como feito no trabalho anterior com a base MNIST.\n",
    "\n",
    "Escolha um MLP com 2 camadas escondidas. Não perca muito tempo variando a arquitetura porque este problema é difícil sem o uso de\n",
    "convoluções e o resultado não será totalmente satisfatório.\n",
    "\n",
    "Você pode usar este código como base para definição da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "\n",
    "#Escolhe-se uma potencia de 2 como numero de unidades escondidas\n",
    "dense_layer_1 = layers.Dense(128, activation='relu')\n",
    "dense_layer_2 = layers.Dense(256, activation='relu')\n",
    "\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que as imagens são achatadas (transformadas em vetor). Substitua as interrogações pelo tamanho desejado das camadas\n",
    "escondidas.\n",
    "\n",
    "Neste problema vamos verificar o fenômeno do \"over-fitting\", e vamos tentar equilibrá-lo pela técnica de parada prematura de treinamento.\n",
    "Assim, dos dados de treinamento precisamos fazer uma nova separação para validação. Quando a acurácia de validação não sobe num dado\n",
    "número de épocas (o parâmetro \"patience\"), o treinamento é interrompido. Este trecho de código pode ser útil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 328.4719 - accuracy: 0.2594 - val_loss: 89.5814 - val_accuracy: 0.3210\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 37.9264 - accuracy: 0.3596 - val_loss: 24.3285 - val_accuracy: 0.2802\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 18.5523 - accuracy: 0.3616 - val_loss: 17.2373 - val_accuracy: 0.3230\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 15.1677 - accuracy: 0.3893 - val_loss: 13.6638 - val_accuracy: 0.3327\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 7.8280 - accuracy: 0.4112 - val_loss: 13.8937 - val_accuracy: 0.2646\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 6.6431 - accuracy: 0.3431 - val_loss: 5.3134 - val_accuracy: 0.2665\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.8646 - accuracy: 0.2569 - val_loss: 3.1148 - val_accuracy: 0.2588\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.6326 - accuracy: 0.2564 - val_loss: 2.9456 - val_accuracy: 0.2490\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.5918 - accuracy: 0.2569 - val_loss: 2.9083 - val_accuracy: 0.2490\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.5871 - accuracy: 0.2569 - val_loss: 2.9113 - val_accuracy: 0.2471\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5848 - accuracy: 0.2569 - val_loss: 2.9060 - val_accuracy: 0.2510\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5834 - accuracy: 0.2574 - val_loss: 2.9043 - val_accuracy: 0.2510\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5828 - accuracy: 0.2579 - val_loss: 2.9011 - val_accuracy: 0.2510\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5821 - accuracy: 0.2579 - val_loss: 2.9004 - val_accuracy: 0.2510\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5819 - accuracy: 0.2579 - val_loss: 2.9041 - val_accuracy: 0.2490\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5821 - accuracy: 0.2569 - val_loss: 2.8929 - val_accuracy: 0.2510\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.5817 - accuracy: 0.2569 - val_loss: 2.8957 - val_accuracy: 0.2510\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5813 - accuracy: 0.2579 - val_loss: 2.9023 - val_accuracy: 0.2510\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.5811 - accuracy: 0.2579 - val_loss: 2.8981 - val_accuracy: 0.2510\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5809 - accuracy: 0.2579 - val_loss: 2.9012 - val_accuracy: 0.2490\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5812 - accuracy: 0.2574 - val_loss: 2.8925 - val_accuracy: 0.2510\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 1s 10ms/step - loss: 1.5807 - accuracy: 0.2584 - val_loss: 2.8960 - val_accuracy: 0.2510\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5807 - accuracy: 0.2584 - val_loss: 2.8967 - val_accuracy: 0.2529\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 1.5826 - accuracy: 0.2579 - val_loss: 2.9203 - val_accuracy: 0.2471\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x140080b20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os parâmetros dados são sugestões. Você agora pode testar o seu modelo, por exemplo, com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 3ms/step - loss: 8.4326 - accuracy: 0.4223\n",
      "Accuracy: 0.42234331369400024\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, procure realizar ajustes, mas não espere um bom desempenho. Como dissemos, é um problema complexo de classificação de\n",
    "imagem, e é difícil fazer o MLP funcionar sozinho. Precisamos de um pré-processamento com base em uma rede convolucional.\n",
    "\n",
    "## 3. Uso da rede VGG16 pré-treinada\n",
    "\n",
    "Lembre-se que a rede VGG usa como bloco básico cascata de convoluções com filtros 3x3, com \"padding\" para que a imagem não seja\n",
    "diminuída, seguida de um \"max pooling\" reduzindo imagens pela metade. O número de mapas vai aumentando e seu tamanho vai diminuindo\n",
    "ao longo de suas 16 camadas. Este é um modelo gigantesco e o treinamento com recursos computacionais modestos levaria dias ou\n",
    "semanas, se é que fosse possível.\n",
    "\n",
    "No entanto, vamos aproveitar uma característica central das grandes redes convolucionais. Elas podem ser usadas como pré-processamento\n",
    "fixo das imagens, mesmo em um novo problema. (Lembre-se, a rede VGG original foi treinada na base ImageNet, que tem muitas categorias de\n",
    "imagem, não apenas flores).\n",
    "\n",
    "O código abaixo realiza o download do modelo treinado, especifica que não será usada a última camada, e que os parâmetros do modelo-base\n",
    "não são ajustáveis.\n",
    "\n",
    "Observe que a classe também tem o método `preprocess_input` para garantir que os dados sejam processados de maneira semelhante ao\n",
    "treinamento original da VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5712,
     "status": "ok",
     "timestamp": 1699914381473,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "WP-b_vl6wH0n",
    "outputId": "9714941f-823b-4d1c-e16a-63bec1ca3bcb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode checar a arquitetura do modelo-base com o método `summary`. Observe que há incríveis 14 milhões de parâmetros no modelo, que\n",
    "felizmente não vamos precisar adaptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1699914387292,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "-J8S_BLpyHrS",
    "outputId": "2ae1bd8b-0a9c-486d-8591-cb155d3cf262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, monte a rede final de forma semelhante ao MLP acima. Use o base_model como primeira camada, seguida de uma camada \"flatten\" (o\n",
    "MLP espera um vetor de entradas), e duas camadas densas. Elas não precisam ser muito grandes. Experimente com 50 e 20, respectivamente,\n",
    "ou algo próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer_final = layers.Flatten() #criando flatten layer da nova MLP\n",
    "\n",
    "#Escolhe-se uma potencia de 2 como numero de unidades escondidas\n",
    "dense_layer_1_final = layers.Dense(50, activation='relu') #criando primeira hidden layer da nova MLP\n",
    "dense_layer_2_final = layers.Dense(20, activation='relu') #criando segunda hidden layer da nova MLP\n",
    "\n",
    "prediction_layer_final = layers.Dense(5, activation='softmax') #criando prediction layer da nova MLP\n",
    "\n",
    "model_final = models.Sequential([\n",
    "    base_model,                      #adicionando camada pre treinada (vgg16) como a primeira camada\n",
    "    flatten_layer_final,\n",
    "    dense_layer_1_final,\n",
    "    dense_layer_2_final,\n",
    "    prediction_layer_final\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volte a treinar e testar o modelo. Mesmo sem efetivamente treinar a rede VGG, ainda temos que passar os dados por ela a cada passo, e o\n",
    "treinamento é um tanto lento. Mas desta vez o problema deve ser resolvido satisfatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 114s 2s/step - loss: 1.6957 - accuracy: 0.5338 - val_loss: 1.0508 - val_accuracy: 0.6323\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 131s 2s/step - loss: 0.6560 - accuracy: 0.7727 - val_loss: 1.0226 - val_accuracy: 0.6654\n",
      "Epoch 3/50\n",
      "30/65 [============>.................] - ETA: 55s - loss: 0.4520 - accuracy: 0.8323"
     ]
    }
   ],
   "source": [
    "model_final.compile(  #criando o modelo final referente a nova MLP\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20,  restore_best_weights=True) #configurando o earlyStopping\n",
    "\n",
    "model_final.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es]) #treinamento da nova MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the final model on the test dataset\n",
    "loss_final, accuracy_final = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Final Accuracy - New MLP:', accuracy_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extras (opcionais)\n",
    "\n",
    "### 4.1. Procure usar outra(s) redes convolucionais como base.\n",
    "### 4.2. No lugar de \"early stopping\", experimente com regularização L1 e L2, e \"dropout\".\n",
    "### 4.3. Procure (ou produza) imagens de algumas flores destas categorias, e teste em seu modelo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyME48Gv3/ej0y2TyzIHwS+K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006e39be5baa48efa5f6a764794ac88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_831cd354b1e940029462d8ca6bbbb2da",
      "placeholder": "​",
      "style": "IPY_MODEL_522ff35e6a3b48789e4f907fa64c6267",
      "value": " 5/5 [00:02&lt;00:00,  1.75 file/s]"
     }
    },
    "1cfef9f82b7d49898c0e8ae5846d4f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "337cb9bac56348ecb28cf342cbf6b204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_efd38707eb0a4ac4b875626cd1f89eed",
       "IPY_MODEL_ccd55a62cefb468c8a2483fd840744ad",
       "IPY_MODEL_006e39be5baa48efa5f6a764794ac88a"
      ],
      "layout": "IPY_MODEL_56a44b3f23a145599a1ead5f4863ae5a"
     }
    },
    "522ff35e6a3b48789e4f907fa64c6267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56a44b3f23a145599a1ead5f4863ae5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "831cd354b1e940029462d8ca6bbbb2da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89dc8b13a44748378faca300b8431842": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccd55a62cefb468c8a2483fd840744ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89dc8b13a44748378faca300b8431842",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7973d1aef464f82a1b24f67028f5720",
      "value": 5
     }
    },
    "e7973d1aef464f82a1b24f67028f5720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efd38707eb0a4ac4b875626cd1f89eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2014973acc24cc0a2b740e53557f1b7",
      "placeholder": "​",
      "style": "IPY_MODEL_1cfef9f82b7d49898c0e8ae5846d4f29",
      "value": "Dl Completed...: 100%"
     }
    },
    "f2014973acc24cc0a2b740e53557f1b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
