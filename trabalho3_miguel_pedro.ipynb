{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trabalho Computacional 3. Rede Convolucional e Transfer Learning\n",
    "\n",
    "## 1. Introdução e Base de Dados\n",
    "\n",
    "Neste trabalho usaremos uma rede convolucional pré-treinada e a aplicaremos em um problema novo. Também experimentaremos com a\n",
    "divisão da base em treinamento, validação e teste, e usaremos validação para o \"early stopping\" na tentativa de controlar o sobre-ajuste.\n",
    "Aproveitamos código publicado na internet por Gabriel Cassimiro.\n",
    "\n",
    "A base de dados é a \"TensorFlow Flowers Dataset\". Ela contém 3670 imagens coloridas de flores pertencentes a uma de 5 classes: Margarida,\n",
    "Dente-de-leão, Rosa, Girassol e Tulipa.\n",
    "\n",
    "Ela pode ser baixada com o código abaixo.\n",
    "\n",
    "Obs.: o módulo `tensorflow_datasets` a princípio não é acessível em instalações Windows. Você pode usar uma instalação local Unix, ou um\n",
    "serviço de núvem como o Google Colab ou Amazon Web Services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install tensorflow_datasets\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "337cb9bac56348ecb28cf342cbf6b204",
      "efd38707eb0a4ac4b875626cd1f89eed",
      "ccd55a62cefb468c8a2483fd840744ad",
      "006e39be5baa48efa5f6a764794ac88a",
      "56a44b3f23a145599a1ead5f4863ae5a",
      "f2014973acc24cc0a2b740e53557f1b7",
      "1cfef9f82b7d49898c0e8ae5846d4f29",
      "89dc8b13a44748378faca300b8431842",
      "e7973d1aef464f82a1b24f67028f5720",
      "831cd354b1e940029462d8ca6bbbb2da",
      "522ff35e6a3b48789e4f907fa64c6267"
     ]
    },
    "executionInfo": {
     "elapsed": 14971,
     "status": "ok",
     "timestamp": 1699914359672,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "Ef2uTTEXumSC",
    "outputId": "ba7d59ee-bfbe-46df-bc71-c915f27d5b57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2569, 150, 150, 3)\n",
      "(1101, 150, 150, 3)\n"
     ]
    }
   ],
   "source": [
    "## Loading images and labels\n",
    "(train_ds, train_labels), (test_ds, test_labels) = tfds.load(\n",
    "    \"tf_flowers\",\n",
    "    split=[\"train[:70%]\", \"train[:30%]\"], ## Train test split\n",
    "    batch_size=-1,\n",
    "    as_supervised=True,  # Include labels\n",
    ")\n",
    "\n",
    "## Resizing images\n",
    "train_ds = tf.image.resize(train_ds, (150, 150))\n",
    "test_ds = tf.image.resize(test_ds, (150, 150))\n",
    "\n",
    "## Transforming labels to correct format\n",
    "train_labels = to_categorical(train_labels, num_classes=5)\n",
    "test_labels = to_categorical(test_labels, num_classes=5)\n",
    "\n",
    "print(train_ds.shape)\n",
    "print(test_ds.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe como foi feita a separação em 70% de dados para treinamento e 30% de dados para teste. Observe também que a base de dados\n",
    "original tem apenas um conjunto \"train\". A separação em treinamento, teste e validação é feita pelo usuário. Por isso a instrução para obter\n",
    "70% do conjunto \"train\" e 30% do conjunto \"train\", o que soa estranho a princípio.\n",
    "\n",
    "## 2. Treinando um MLP\n",
    "\n",
    "Use esta base de dados para treinar um MLP , como feito no trabalho anterior com a base MNIST.\n",
    "\n",
    "Escolha um MLP com 2 camadas escondidas. Não perca muito tempo variando a arquitetura porque este problema é difícil sem o uso de\n",
    "convoluções e o resultado não será totalmente satisfatório.\n",
    "\n",
    "Você pode usar este código como base para definição da rede:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "flatten_layer = layers.Flatten()\n",
    "\n",
    "#Escolhe-se uma potencia de 2 como numero de unidades escondidas\n",
    "dense_layer_1 = layers.Dense(128, activation='relu')\n",
    "dense_layer_2 = layers.Dense(256, activation='relu')\n",
    "\n",
    "prediction_layer = layers.Dense(5, activation='softmax')\n",
    "\n",
    "model = models.Sequential([\n",
    "    flatten_layer,\n",
    "    dense_layer_1,\n",
    "    dense_layer_2,\n",
    "    prediction_layer\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe que as imagens são achatadas (transformadas em vetor). Substitua as interrogações pelo tamanho desejado das camadas\n",
    "escondidas.\n",
    "\n",
    "Neste problema vamos verificar o fenômeno do \"over-fitting\", e vamos tentar equilibrá-lo pela técnica de parada prematura de treinamento.\n",
    "Assim, dos dados de treinamento precisamos fazer uma nova separação para validação. Quando a acurácia de validação não sobe num dado\n",
    "número de épocas (o parâmetro \"patience\"), o treinamento é interrompido. Este trecho de código pode ser útil:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 236.5558 - accuracy: 0.2662 - val_loss: 66.5867 - val_accuracy: 0.3249\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 46.6983 - accuracy: 0.3499 - val_loss: 60.4864 - val_accuracy: 0.2802\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 25.7189 - accuracy: 0.3533 - val_loss: 26.8136 - val_accuracy: 0.2510\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 13.2509 - accuracy: 0.3698 - val_loss: 22.5800 - val_accuracy: 0.2977\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 9.7263 - accuracy: 0.4029 - val_loss: 12.3561 - val_accuracy: 0.3054\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 6.3641 - accuracy: 0.4034 - val_loss: 11.7199 - val_accuracy: 0.3405\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 1s 11ms/step - loss: 9.1316 - accuracy: 0.3552 - val_loss: 6.5508 - val_accuracy: 0.1984\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 2.2853 - accuracy: 0.2482 - val_loss: 2.5142 - val_accuracy: 0.2471\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 1.6169 - accuracy: 0.2521 - val_loss: 2.4148 - val_accuracy: 0.2510\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 1.6165 - accuracy: 0.2526 - val_loss: 2.3650 - val_accuracy: 0.2510\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 1s 12ms/step - loss: 1.6090 - accuracy: 0.2526 - val_loss: 2.4201 - val_accuracy: 0.2490\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5898 - accuracy: 0.2530 - val_loss: 2.4564 - val_accuracy: 0.2490\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 1s 14ms/step - loss: 1.5936 - accuracy: 0.2530 - val_loss: 2.4034 - val_accuracy: 0.2510\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5899 - accuracy: 0.2530 - val_loss: 2.4008 - val_accuracy: 0.2510\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5898 - accuracy: 0.2530 - val_loss: 2.4013 - val_accuracy: 0.2510\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4012 - val_accuracy: 0.2510\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5915 - accuracy: 0.2530 - val_loss: 2.4011 - val_accuracy: 0.2510\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5899 - accuracy: 0.2530 - val_loss: 2.4021 - val_accuracy: 0.2510\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5898 - accuracy: 0.2530 - val_loss: 2.4030 - val_accuracy: 0.2510\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4026 - val_accuracy: 0.2510\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4018 - val_accuracy: 0.2510\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4026 - val_accuracy: 0.2510\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5898 - accuracy: 0.2530 - val_loss: 2.4025 - val_accuracy: 0.2510\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4027 - val_accuracy: 0.2510\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5896 - accuracy: 0.2530 - val_loss: 2.4033 - val_accuracy: 0.2510\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 1s 13ms/step - loss: 1.5897 - accuracy: 0.2530 - val_loss: 2.4035 - val_accuracy: 0.2510\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x110074b20>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20,  restore_best_weights=True)\n",
    "\n",
    "model.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os parâmetros dados são sugestões. Você agora pode testar o seu modelo, por exemplo, com:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 0s 4ms/step - loss: 7.7043 - accuracy: 0.4260\n",
      "Accuracy: 0.42597639560699463\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model on the test dataset\n",
    "loss, accuracy = model.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mais uma vez, procure realizar ajustes, mas não espere um bom desempenho. Como dissemos, é um problema complexo de classificação de\n",
    "imagem, e é difícil fazer o MLP funcionar sozinho. Precisamos de um pré-processamento com base em uma rede convolucional.\n",
    "\n",
    "## 3. Uso da rede VGG16 pré-treinada\n",
    "\n",
    "Lembre-se que a rede VGG usa como bloco básico cascata de convoluções com filtros 3x3, com \"padding\" para que a imagem não seja\n",
    "diminuída, seguida de um \"max pooling\" reduzindo imagens pela metade. O número de mapas vai aumentando e seu tamanho vai diminuindo\n",
    "ao longo de suas 16 camadas. Este é um modelo gigantesco e o treinamento com recursos computacionais modestos levaria dias ou\n",
    "semanas, se é que fosse possível.\n",
    "\n",
    "No entanto, vamos aproveitar uma característica central das grandes redes convolucionais. Elas podem ser usadas como pré-processamento\n",
    "fixo das imagens, mesmo em um novo problema. (Lembre-se, a rede VGG original foi treinada na base ImageNet, que tem muitas categorias de\n",
    "imagem, não apenas flores).\n",
    "\n",
    "O código abaixo realiza o download do modelo treinado, especifica que não será usada a última camada, e que os parâmetros do modelo-base\n",
    "não são ajustáveis.\n",
    "\n",
    "Observe que a classe também tem o método `preprocess_input` para garantir que os dados sejam processados de maneira semelhante ao\n",
    "treinamento original da VGG16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5712,
     "status": "ok",
     "timestamp": 1699914381473,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "WP-b_vl6wH0n",
    "outputId": "9714941f-823b-4d1c-e16a-63bec1ca3bcb"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "## Loading VGG16 model\n",
    "base_model = VGG16(weights=\"imagenet\", include_top=False, input_shape=train_ds[0].shape)\n",
    "base_model.trainable = False ## Not trainable weights\n",
    "\n",
    "## Preprocessing input\n",
    "train_ds = preprocess_input(train_ds)\n",
    "test_ds = preprocess_input(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Você pode checar a arquitetura do modelo-base com o método `summary`. Observe que há incríveis 14 milhões de parâmetros no modelo, que\n",
    "felizmente não vamos precisar adaptar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 355,
     "status": "ok",
     "timestamp": 1699914387292,
     "user": {
      "displayName": "Alexandre Romariz",
      "userId": "08294063804806616136"
     },
     "user_tz": 180
    },
    "id": "-J8S_BLpyHrS",
    "outputId": "2ae1bd8b-0a9c-486d-8591-cb155d3cf262"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"vgg16\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 150, 150, 3)]     0         \n",
      "                                                                 \n",
      " block1_conv1 (Conv2D)       (None, 150, 150, 64)      1792      \n",
      "                                                                 \n",
      " block1_conv2 (Conv2D)       (None, 150, 150, 64)      36928     \n",
      "                                                                 \n",
      " block1_pool (MaxPooling2D)  (None, 75, 75, 64)        0         \n",
      "                                                                 \n",
      " block2_conv1 (Conv2D)       (None, 75, 75, 128)       73856     \n",
      "                                                                 \n",
      " block2_conv2 (Conv2D)       (None, 75, 75, 128)       147584    \n",
      "                                                                 \n",
      " block2_pool (MaxPooling2D)  (None, 37, 37, 128)       0         \n",
      "                                                                 \n",
      " block3_conv1 (Conv2D)       (None, 37, 37, 256)       295168    \n",
      "                                                                 \n",
      " block3_conv2 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_conv3 (Conv2D)       (None, 37, 37, 256)       590080    \n",
      "                                                                 \n",
      " block3_pool (MaxPooling2D)  (None, 18, 18, 256)       0         \n",
      "                                                                 \n",
      " block4_conv1 (Conv2D)       (None, 18, 18, 512)       1180160   \n",
      "                                                                 \n",
      " block4_conv2 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_conv3 (Conv2D)       (None, 18, 18, 512)       2359808   \n",
      "                                                                 \n",
      " block4_pool (MaxPooling2D)  (None, 9, 9, 512)         0         \n",
      "                                                                 \n",
      " block5_conv1 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv2 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_conv3 (Conv2D)       (None, 9, 9, 512)         2359808   \n",
      "                                                                 \n",
      " block5_pool (MaxPooling2D)  (None, 4, 4, 512)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14714688 (56.13 MB)\n",
      "Trainable params: 0 (0.00 Byte)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora, monte a rede final de forma semelhante ao MLP acima. Use o base_model como primeira camada, seguida de uma camada \"flatten\" (o\n",
    "MLP espera um vetor de entradas), e duas camadas densas. Elas não precisam ser muito grandes. Experimente com 50 e 20, respectivamente,\n",
    "ou algo próximo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flatten_layer_final = layers.Flatten() #criando flatten layer da nova MLP\n",
    "\n",
    "#Escolhe-se uma potencia de 2 como numero de unidades escondidas\n",
    "dense_layer_1_final = layers.Dense(50, activation='relu') #criando primeira hidden layer da nova MLP\n",
    "dense_layer_2_final = layers.Dense(20, activation='relu') #criando segunda hidden layer da nova MLP\n",
    "\n",
    "prediction_layer_final = layers.Dense(5, activation='softmax') #criando prediction layer da nova MLP\n",
    "\n",
    "model_final = models.Sequential([\n",
    "    base_model,                      #adicionando camada pre treinada (vgg16) como a primeira camada\n",
    "    flatten_layer_final,\n",
    "    dense_layer_1_final,\n",
    "    dense_layer_2_final,\n",
    "    prediction_layer_final\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Volte a treinar e testar o modelo. Mesmo sem efetivamente treinar a rede VGG, ainda temos que passar os dados por ela a cada passo, e o\n",
    "treinamento é um tanto lento. Mas desta vez o problema deve ser resolvido satisfatoriamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "65/65 [==============================] - 124s 2s/step - loss: 1.6479 - accuracy: 0.4569 - val_loss: 1.0563 - val_accuracy: 0.5739\n",
      "Epoch 2/50\n",
      "65/65 [==============================] - 116s 2s/step - loss: 0.8863 - accuracy: 0.6696 - val_loss: 0.9371 - val_accuracy: 0.6576\n",
      "Epoch 3/50\n",
      "65/65 [==============================] - 114s 2s/step - loss: 0.6437 - accuracy: 0.7484 - val_loss: 0.8989 - val_accuracy: 0.6809\n",
      "Epoch 4/50\n",
      "65/65 [==============================] - 115s 2s/step - loss: 0.5359 - accuracy: 0.7869 - val_loss: 0.9989 - val_accuracy: 0.6829\n",
      "Epoch 5/50\n",
      "65/65 [==============================] - 119s 2s/step - loss: 0.4412 - accuracy: 0.8224 - val_loss: 1.0106 - val_accuracy: 0.6751\n",
      "Epoch 6/50\n",
      "65/65 [==============================] - 119s 2s/step - loss: 0.3672 - accuracy: 0.8472 - val_loss: 1.1132 - val_accuracy: 0.6751\n",
      "Epoch 7/50\n",
      "65/65 [==============================] - 695s 11s/step - loss: 0.2963 - accuracy: 0.8774 - val_loss: 1.1509 - val_accuracy: 0.6693\n",
      "Epoch 8/50\n",
      "65/65 [==============================] - 98s 2s/step - loss: 0.2659 - accuracy: 0.8900 - val_loss: 1.3231 - val_accuracy: 0.6732\n",
      "Epoch 9/50\n",
      "65/65 [==============================] - 100s 2s/step - loss: 0.2396 - accuracy: 0.8973 - val_loss: 1.4251 - val_accuracy: 0.6809\n",
      "Epoch 10/50\n",
      "65/65 [==============================] - 102s 2s/step - loss: 0.2204 - accuracy: 0.9071 - val_loss: 1.3638 - val_accuracy: 0.6732\n",
      "Epoch 11/50\n",
      "65/65 [==============================] - 111s 2s/step - loss: 0.1867 - accuracy: 0.9236 - val_loss: 1.5204 - val_accuracy: 0.6712\n",
      "Epoch 12/50\n",
      "65/65 [==============================] - 117s 2s/step - loss: 0.1715 - accuracy: 0.9333 - val_loss: 1.3687 - val_accuracy: 0.6712\n",
      "Epoch 13/50\n",
      "65/65 [==============================] - 120s 2s/step - loss: 0.1848 - accuracy: 0.9231 - val_loss: 1.5221 - val_accuracy: 0.6887\n",
      "Epoch 14/50\n",
      "65/65 [==============================] - 1016s 16s/step - loss: 0.1624 - accuracy: 0.9377 - val_loss: 1.4556 - val_accuracy: 0.6907\n",
      "Epoch 15/50\n",
      "65/65 [==============================] - 99s 2s/step - loss: 0.1420 - accuracy: 0.9440 - val_loss: 1.6211 - val_accuracy: 0.6790\n",
      "Epoch 16/50\n",
      "65/65 [==============================] - 100s 2s/step - loss: 0.1225 - accuracy: 0.9494 - val_loss: 1.8410 - val_accuracy: 0.6965\n",
      "Epoch 17/50\n",
      "65/65 [==============================] - 101s 2s/step - loss: 0.1178 - accuracy: 0.9533 - val_loss: 1.7819 - val_accuracy: 0.6556\n",
      "Epoch 18/50\n",
      "65/65 [==============================] - 106s 2s/step - loss: 0.1226 - accuracy: 0.9596 - val_loss: 1.7105 - val_accuracy: 0.6848\n",
      "Epoch 19/50\n",
      "65/65 [==============================] - 111s 2s/step - loss: 0.0844 - accuracy: 0.9655 - val_loss: 1.9388 - val_accuracy: 0.6693\n",
      "Epoch 20/50\n",
      "65/65 [==============================] - 116s 2s/step - loss: 0.0781 - accuracy: 0.9674 - val_loss: 1.6928 - val_accuracy: 0.6946\n",
      "Epoch 21/50\n",
      "65/65 [==============================] - 116s 2s/step - loss: 0.0701 - accuracy: 0.9723 - val_loss: 1.8772 - val_accuracy: 0.6829\n",
      "Epoch 22/50\n",
      "65/65 [==============================] - 116s 2s/step - loss: 0.0582 - accuracy: 0.9752 - val_loss: 1.8440 - val_accuracy: 0.6907\n",
      "Epoch 23/50\n",
      "65/65 [==============================] - 118s 2s/step - loss: 0.0915 - accuracy: 0.9650 - val_loss: 1.9975 - val_accuracy: 0.6829\n",
      "Epoch 24/50\n",
      "65/65 [==============================] - 121s 2s/step - loss: 0.1005 - accuracy: 0.9620 - val_loss: 1.8093 - val_accuracy: 0.6615\n",
      "Epoch 25/50\n",
      "65/65 [==============================] - 123s 2s/step - loss: 0.1186 - accuracy: 0.9582 - val_loss: 2.0419 - val_accuracy: 0.6829\n",
      "Epoch 26/50\n",
      "65/65 [==============================] - 1977s 31s/step - loss: 0.1118 - accuracy: 0.9557 - val_loss: 2.3061 - val_accuracy: 0.6790\n",
      "Epoch 27/50\n",
      "65/65 [==============================] - 668s 10s/step - loss: 0.1104 - accuracy: 0.9616 - val_loss: 2.5015 - val_accuracy: 0.6790\n",
      "Epoch 28/50\n",
      "65/65 [==============================] - 96s 1s/step - loss: 0.1043 - accuracy: 0.9616 - val_loss: 2.2732 - val_accuracy: 0.6868\n",
      "Epoch 29/50\n",
      "65/65 [==============================] - 99s 2s/step - loss: 0.0863 - accuracy: 0.9689 - val_loss: 2.1700 - val_accuracy: 0.6984\n",
      "Epoch 30/50\n",
      "65/65 [==============================] - 100s 2s/step - loss: 0.0845 - accuracy: 0.9703 - val_loss: 2.2856 - val_accuracy: 0.6751\n",
      "Epoch 31/50\n",
      "65/65 [==============================] - 101s 2s/step - loss: 0.1273 - accuracy: 0.9606 - val_loss: 2.4484 - val_accuracy: 0.6673\n",
      "Epoch 32/50\n",
      "65/65 [==============================] - 103s 2s/step - loss: 0.0610 - accuracy: 0.9810 - val_loss: 2.6721 - val_accuracy: 0.6770\n",
      "Epoch 33/50\n",
      "65/65 [==============================] - 110s 2s/step - loss: 0.0641 - accuracy: 0.9805 - val_loss: 2.9478 - val_accuracy: 0.6654\n",
      "Epoch 34/50\n",
      "65/65 [==============================] - 110s 2s/step - loss: 0.0447 - accuracy: 0.9815 - val_loss: 2.5745 - val_accuracy: 0.6809\n",
      "Epoch 35/50\n",
      "65/65 [==============================] - 1013s 16s/step - loss: 0.0333 - accuracy: 0.9878 - val_loss: 2.4077 - val_accuracy: 0.6770\n",
      "Epoch 36/50\n",
      "65/65 [==============================] - 128s 2s/step - loss: 0.0240 - accuracy: 0.9903 - val_loss: 2.6279 - val_accuracy: 0.6809\n",
      "Epoch 37/50\n",
      "65/65 [==============================] - 131s 2s/step - loss: 0.0225 - accuracy: 0.9908 - val_loss: 2.9288 - val_accuracy: 0.6751\n",
      "Epoch 38/50\n",
      "65/65 [==============================] - 171s 3s/step - loss: 0.0379 - accuracy: 0.9873 - val_loss: 2.5678 - val_accuracy: 0.6946\n",
      "Epoch 39/50\n",
      "65/65 [==============================] - 186s 3s/step - loss: 0.0412 - accuracy: 0.9859 - val_loss: 3.3865 - val_accuracy: 0.6595\n",
      "Epoch 40/50\n",
      "65/65 [==============================] - 2012s 31s/step - loss: 0.0548 - accuracy: 0.9839 - val_loss: 3.1008 - val_accuracy: 0.6770\n",
      "Epoch 41/50\n",
      "65/65 [==============================] - 1044s 16s/step - loss: 0.0781 - accuracy: 0.9820 - val_loss: 3.1166 - val_accuracy: 0.6790\n",
      "Epoch 42/50\n",
      "65/65 [==============================] - 1014s 16s/step - loss: 0.0698 - accuracy: 0.9815 - val_loss: 3.0918 - val_accuracy: 0.6420\n",
      "Epoch 43/50\n",
      "65/65 [==============================] - 94s 1s/step - loss: 0.0753 - accuracy: 0.9757 - val_loss: 2.7863 - val_accuracy: 0.6751\n",
      "Epoch 44/50\n",
      "65/65 [==============================] - 98s 2s/step - loss: 0.0842 - accuracy: 0.9737 - val_loss: 2.5524 - val_accuracy: 0.6829\n",
      "Epoch 45/50\n",
      "65/65 [==============================] - 99s 2s/step - loss: 0.0562 - accuracy: 0.9839 - val_loss: 2.7139 - val_accuracy: 0.7004\n",
      "Epoch 46/50\n",
      "65/65 [==============================] - 100s 2s/step - loss: 0.0309 - accuracy: 0.9888 - val_loss: 2.8613 - val_accuracy: 0.6829\n",
      "Epoch 47/50\n",
      "65/65 [==============================] - 100s 2s/step - loss: 0.0316 - accuracy: 0.9898 - val_loss: 2.8143 - val_accuracy: 0.6673\n",
      "Epoch 48/50\n",
      "65/65 [==============================] - 101s 2s/step - loss: 0.0200 - accuracy: 0.9922 - val_loss: 3.0314 - val_accuracy: 0.6829\n",
      "Epoch 49/50\n",
      "65/65 [==============================] - 103s 2s/step - loss: 0.0126 - accuracy: 0.9937 - val_loss: 2.9798 - val_accuracy: 0.6790\n",
      "Epoch 50/50\n",
      "65/65 [==============================] - 108s 2s/step - loss: 0.0118 - accuracy: 0.9946 - val_loss: 3.0119 - val_accuracy: 0.6751\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x1144b81c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_final.compile(  #criando o modelo final referente a nova MLP\n",
    "    optimizer='adam',\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy'],\n",
    ")\n",
    "\n",
    "\n",
    "es = EarlyStopping(monitor='val_accuracy', mode='max', patience=20,  restore_best_weights=True) #configurando o earlyStopping\n",
    "\n",
    "model_final.fit(train_ds, train_labels, epochs=50, validation_split=0.2, batch_size=32, callbacks=[es]) #treinamento da nova MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 39s 1s/step - loss: 0.0088 - accuracy: 0.9964\n",
      "Final Accuracy - New MLP: 0.9963669180870056\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the final model on the test dataset\n",
    "loss_final, accuracy_final = model_final.evaluate(test_ds, test_labels)\n",
    "\n",
    "# Print the accuracy\n",
    "print('Final Accuracy - New MLP:', accuracy_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota-se que o desempenho da nova MLP, tendo nova camada de entrada a camada base_model pré-treinada, é consideravelmente superior ao anterior, (a julgar pelas métricas mostradas pelo próprio processo de treinamento, em especial a acurácia, a qual passou dos 99 pontos percentuais). Nesse sentido, a eficiência do processo denominado \"Transfer Learning\" fica demonstrada."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extras (opcionais)\n",
    "\n",
    "### 4.1. Procure usar outra(s) redes convolucionais como base.\n",
    "### 4.2. No lugar de \"early stopping\", experimente com regularização L1 e L2, e \"dropout\".\n",
    "### 4.3. Procure (ou produza) imagens de algumas flores destas categorias, e teste em seu modelo."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyME48Gv3/ej0y2TyzIHwS+K",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "006e39be5baa48efa5f6a764794ac88a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_831cd354b1e940029462d8ca6bbbb2da",
      "placeholder": "​",
      "style": "IPY_MODEL_522ff35e6a3b48789e4f907fa64c6267",
      "value": " 5/5 [00:02&lt;00:00,  1.75 file/s]"
     }
    },
    "1cfef9f82b7d49898c0e8ae5846d4f29": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "337cb9bac56348ecb28cf342cbf6b204": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_efd38707eb0a4ac4b875626cd1f89eed",
       "IPY_MODEL_ccd55a62cefb468c8a2483fd840744ad",
       "IPY_MODEL_006e39be5baa48efa5f6a764794ac88a"
      ],
      "layout": "IPY_MODEL_56a44b3f23a145599a1ead5f4863ae5a"
     }
    },
    "522ff35e6a3b48789e4f907fa64c6267": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "56a44b3f23a145599a1ead5f4863ae5a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "831cd354b1e940029462d8ca6bbbb2da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "89dc8b13a44748378faca300b8431842": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ccd55a62cefb468c8a2483fd840744ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_89dc8b13a44748378faca300b8431842",
      "max": 5,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e7973d1aef464f82a1b24f67028f5720",
      "value": 5
     }
    },
    "e7973d1aef464f82a1b24f67028f5720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "efd38707eb0a4ac4b875626cd1f89eed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f2014973acc24cc0a2b740e53557f1b7",
      "placeholder": "​",
      "style": "IPY_MODEL_1cfef9f82b7d49898c0e8ae5846d4f29",
      "value": "Dl Completed...: 100%"
     }
    },
    "f2014973acc24cc0a2b740e53557f1b7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
