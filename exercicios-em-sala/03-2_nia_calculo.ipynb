{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74ac336d-9913-4abd-8d1f-5f37a6d2a15d",
   "metadata": {},
   "source": [
    "# Operações de Álgebra Linear e Cálculo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7647118a-76a1-44fa-a281-928eac731f81",
   "metadata": {},
   "source": [
    "## 1. Operações de Álgebra Linear "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3003e-338e-4c02-8efa-5445073c708b",
   "metadata": {},
   "source": [
    "### 1.1. Somas e Reduções de Dimensionalidade ; Transmissão (\"BroadCasting\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554936a5-e62c-420e-928c-570e5fa687dd",
   "metadata": {},
   "source": [
    "Vamos importar o tensorflow e criar uma matriz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "324c750b-d633-404e-b75c-c7d30a8ddb6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-03 17:37:55.950944: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-03 17:37:57.058707: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2024-04-03 17:37:58.330909: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-04-03 17:37:58.344678: W tensorflow/core/common_runtime/gpu/gpu_device.cc:1956] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=int32, numpy=\n",
       "array([[0, 1, 2],\n",
       "       [3, 4, 5]], dtype=int32)>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "A=tf.reshape(tf.range(6),(2,3))\n",
    "A"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd888ee-cd79-46ce-94a9-68733e09c5d9",
   "metadata": {},
   "source": [
    "Observe que é um array bidimensional 2x3. Frequentemente precisamos somar as linhas ou colunas de matrizes (para fazer normalizações, por exemplo). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f33ac1b-0cf0-4e5b-a58c-6625fb12239c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=int32, numpy=15>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83eac30-c7e8-4981-8819-94944fee3325",
   "metadata": {},
   "source": [
    "O ``reduce_sum``, sem parâmetros, soma todos os elementos da matriz A, e devolve um escalar (dimensão 0). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfc75bf8-a171-4331-aeac-5776bb813dc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 5, 7], dtype=int32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.reduce_sum(A,axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dea618-6bef-4951-8003-d44466032f7f",
   "metadata": {},
   "source": [
    "Indicando-se a dimensão 0 (axis), apenas as linhas são colapsadas, isto é, temos um vetor de tamanho 3 (array monodimensional) constituído pela soma de cada coluna. Observe como a função ``reduce_sum`` a princípio diminui a dimensionalidade do objeto. A função ``reduce_mean`` tem comportamento semelhante, mas com a média (e não a soma) de elementos. \n",
    "\n",
    "Esta diminuição de dimensionalidade nem sempre é desejável. Digamos que calculemos a soma das linhas de A, para normalizá-las (fazer toda linha ter soma 1). Não podemos fazer simplesmente ``A/reduce_sum(A,axis=1)`` porque estaríamos dividindo uma matriz (dimensão 2) por um vetor (dimensão 1). A solução é usar a opção ``keepdims=True`` para que as dimensões de tamanho 1 não sejam colapsadas. Em outras palavras, para que esta soma de linhas seja representada em uma matriz 2x1 e não um vetor 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31f49992-de92-4a63-a7d7-258e3b6a3fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(2,), dtype=int32, numpy=array([ 3, 12], dtype=int32)>,\n",
       " <tf.Tensor: shape=(2, 1), dtype=int32, numpy=\n",
       " array([[ 3],\n",
       "        [12]], dtype=int32)>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SomaLinhas=tf.reduce_sum(A,axis=1)\n",
    "SomaLinhas2=tf.reduce_sum(A,axis=1,keepdims=True)\n",
    "SomaLinhas,SomaLinhas2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "169d6cd8-403c-44d1-b50f-b454a94fd360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 3), dtype=float64, numpy=\n",
       "array([[0.        , 0.33333333, 0.66666667],\n",
       "       [0.25      , 0.33333333, 0.41666667]])>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A/SomaLinhas2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba75d0c-5a44-4135-a286-89493ca18e54",
   "metadata": {},
   "source": [
    "Mas dissemos anteriormente que a divisão / é realizada elemento a elemento. A e SomaLinhas 2 têm dimensão 2, mas os tamanhos não são os mesmos (2x3 contra 2x1). Como a operação pôde ser realizada corretamente? A resposta está na importante característica de **Broadcasting** (transmissão) pressuposta nas operações matriciais no TensorFlow. Antes de realizar a operação, as dimensões de tamanho 1 são replicadas para que tenhamos tamanhos compatíveis. Em outras palavras, interpreta-se que o que se deseja é fazer 3 cópias (em colunas) da matriz 2x1 para que se tenha uma matriz 2x3. Observe que a operação é exatamente o que queríamos: a soma das linhas é 1. \n",
    "\n",
    "\"Broadcasting\" será muito útil na manipulação de grandes tabelas de dados em Aprendizado de Máquina. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13ab467f-25f6-494a-8e23-d6a9a1abbb0f",
   "metadata": {},
   "source": [
    "### 1.2. Produto interno, produto matriz-vetor e matriz-matriz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8c838c-1469-409f-8fbc-e4d0f4bbfd57",
   "metadata": {},
   "source": [
    "A função ``tf.tensordot`` realiza o produto interno de vetores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f74322e8-3306-4bf6-b4d9-4b92be70fac5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>,\n",
       " <tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 1, 2], dtype=int32)>,\n",
       " <tf.Tensor: shape=(), dtype=int32, numpy=5>)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x,y=tf.range(3),tf.range(3)\n",
    "x,y,tf.tensordot(x,y,axes=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d26801-1d78-426e-bff6-60357f319632",
   "metadata": {},
   "source": [
    "Já a função ``tf.matmul``realiza o produto matriz x vetor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a51a75b3-9fb4-483f-99ed-be5a77e14aca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-0.5663028 , -0.75543094,  0.44377095],\n",
       "       [-1.4696789 , -0.12964225, -1.2041924 ],\n",
       "       [-0.8877599 , -0.5225164 ,  1.6861261 ]], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W=tf.random.normal(shape=(3,3))\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51f5efa6-458b-46ad-8e34-c463b120a0e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-0.14805588,  1.0885344 ,  0.4512012 ],\n",
       "       [-1.6853718 , -0.34593436,  1.0268487 ],\n",
       "       [-0.25259694,  1.6683149 ,  0.48578602]], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y=tf.random.normal(shape=(3,3))\n",
    "tf.matmul(W,Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb8d182-fdb9-4039-9c8b-e06aa17dc356",
   "metadata": {},
   "source": [
    "### 1.3. Normas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cafe7b-f0e6-48bd-ad28-a785ef9745ee",
   "metadata": {},
   "source": [
    "A função ``tf.norm`` calcula a norma L2 (ou euclidiana) de um vetor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "086212d6-87fe-4947-8bba-45db85cc07f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=tf.constant([3.0,4.0])\n",
    "tf.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62864e0b-8f13-4189-a842-3f92d02f6e78",
   "metadata": {},
   "source": [
    "O \".0\" ao final faz com que A seja um vetor de ``float32``. A função ``tf.norm``não aceita inteiros como argumentos. É possível contornar isso com a função ``tf.cast`` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87adf1fa-5070-402b-9601-ad1f6c43e878",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=5.0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = tf.constant([3, 4])\n",
    "tf.norm(tf.cast(A, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7f9d5b2-7ce6-4b72-b82e-ca59b38a6e43",
   "metadata": {},
   "source": [
    "Se aplicada a uma matriz, ``tf.norm`` apresenta a norma de Frobenius da matriz (raiz quadrada da soma dos quadrados de todos os elementos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "18a8cb0e-319c-4ac4-9b62-823f910d7d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=3.0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A=tf.ones((3,3))\n",
    "tf.norm(A)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab88d21-12b7-4191-bd81-7bf4f28bd151",
   "metadata": {},
   "source": [
    "Observe que a função ``tf.ones`` devolve um vetor de ``float32``, não sendo necessário o \"casting\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb619965-1268-4c9b-ab16-71bc3dbd954e",
   "metadata": {},
   "source": [
    "## 2. Cálculo: Diferenciação automática"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e34c73-db63-4a6d-9b70-d2d4c27696e8",
   "metadata": {},
   "source": [
    "Na Figura abaixo, temos um neurônio artificial com entradas $\\vec{x}$, pesos sinápticos $\\vec{w}$ e saída $y$.\n",
    "\n",
    "![Neuronio Artificial](./Neuronio.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dabb59cf-23c8-44a5-8074-9374c7ac770e",
   "metadata": {},
   "source": [
    "Diferente do modelo que vimos aula passada, este neurônio não computa apenas o produto interno entre $\\vec{w}$ e $\\vec{x}$ (ou dito de outra forma, faz a soma das entradas $\\vec{x}$ ponderada pelos pesos $\\vec{w}$). Ele também passa a soma resultante por uma função não-linear, conhecida como logística ou sigmoide: \n",
    "\\begin{equation*}\n",
    "y=\\frac{1}{1+\\exp(-s)}\n",
    "\\end{equation*},\n",
    "onde $s$ é a soma.\n",
    "\n",
    "Frequentemente queremos saber qual a derivada de $y$ com relação a cada peso $w_{i}$. Isto será usado para modificar $w$ de acordo com os erros que a rede neural comete. \n",
    "\n",
    "TensorFlow permite o cálculo automático das derivadas de variáveis computadas. \n",
    "\n",
    "Primeiro, vamos simular um passo do cálculo da saída do neurônio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d388fef3-d6bb-4cef-83a8-7c70eb9f7ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=tf.Variable(tf.constant([1.0,-2.0,3.0]))\n",
    "w=tf.Variable(tf.constant([0.2,-0.2,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51899724-587e-49f6-ae08-0cf175ef6242",
   "metadata": {},
   "source": [
    "Encontramos pela primeira vez a função ``tf.Variable``. Esta é uma ferramenta de economia de memória. Python usa gerenciamento dinâmico de memória, o que significa que ele vai alocando memória ao longo da execução. Isto pode ser útil no gerenciamento de memória (podemos ir descartando a memória que não será mais usada), mas se usado sem cuidado pode ter o efeito contrário. Quando fazemos ``Y=X+Y`` alocamos memória para `X+Y` mesmo sendo o objetivo apenas atualizar `Y`, por exemplo. \n",
    "\n",
    "Para garantir que algo que será calculado várias vezes (como os parâmetros de uma rede neural) sejam alocados em uma mesma porção de memória, existe a função ``tf.Variable``. Ela recebe o valor inicial da variável e, futuramente, o valor pode ser mudado sem alocar nova memória. \n",
    "\n",
    "Resolvida a questão da alocação, podemos calcular o valor da saída. Mas queremos que este cálculo seja usado para que o gradiente da saída em relação aos pesos $\\vec{w}$ seja calculado automaticamente. Para isto, colocamos o código onde a conta é feita sob a declaração ``with tf.GradientTape() as t:``. A declaração ``with`` em Python define um encapsulamento e um contexto para o código. É comum para arquivos (veja o notebook/caderno anterior), que podem ser fechados após a leitura, ou justamente para declarar que as operações devem ser memorizadas para que se compute o gradiente, como aqui. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a800e876-cad5-427c-adf1-64409516d2f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(), dtype=float32, numpy=0.973403>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.GradientTape() as t:\n",
    "    y=tf.sigmoid(tf.tensordot(x,w,axes=1))\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a8f7ad-9a0a-425a-b5df-0f4ba3fcf4df",
   "metadata": {},
   "source": [
    "Observe que a computação está correta. O produto interno nos fornece $1\\times 0.2+(-2)\\times(-0.2)+3\\times 1 = 3,6$. Verifique que a função logística, definida acima, vale $0,97$ para $s=3,6$. \n",
    "\n",
    "Agora vamos ao gradiente. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c7d898ae-af1e-4621-9281-e6e12c3945ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.02588962, -0.05177924,  0.07766887], dtype=float32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dydw = t.gradient(y, w)\n",
    "dydw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fa5b2f-839c-4340-ac2a-669133ba0ba0",
   "metadata": {},
   "source": [
    "De novo, verifique que a computação está correta. Para calcular a derivada parcial de $y$ com relação a cada $w_{i}$ aplique a regra da cadeia. $$\\frac{\\partial y}{\\partial w_{i}}=\\frac{dy}{ds}\\cdot\\frac{\\partial s}{w_{i}}$$\n",
    "\n",
    "Compute a derivada de $y$ com relação a $s$, verificando que vale $s(1-s)$. O outro fator, já que $s$ é simplesmente a soma ponderada, vale $x_{i}$. Multiplique os fatores e verifique o resultado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f853c28-8b3d-47ae-ad08-22d92b5f7062",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
